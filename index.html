<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yipeng Gao</title>

    <meta name="author" content="Yipeng Gao">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yipeng Gao
                </p>
                <p> I am a first-year Ph.D. student in Computer Science at <a href="https://www.cs.usc.edu/">University of Southern California</a>, advised by Prof. <a href="http://ilab.usc.edu/itti/">Laurent Itti</a>.   
                </p>
                <p>
                  Previously, I obtained an M.S. degree at <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a> (SYSU) and a B.E. degree at the <a href="https://www.scut.edu.cn/en/"> South China University of Technology</a>.
                  I was fortunate to have an in-person internship with Prof. <a href="https://cihangxie.github.io/">Cihang Xie</a> at the <a href="https://www.ucsc.edu/"> University of California, Santa Cruz</a>.
                </p>
                <p>
                  My research interests mainly lie around Computer Vision, with the goal of developing general and generalizable vision systems.
                  Nowadays, I focus on vision-language foundation models and their applications.
                  I believe it is an essential tool for facilitating people's interaction with modern vision systems and enhancing the quality of their lives.
                  Now, I am committing to take a small step toward this vision.
                </p>
                <p>
                  <span>I am always open to research discussions and collaborations :).
                    Feel free to reach out to me with any questions. </span>
                </p>
                <p style="text-align:center">
                  <a href="gaoyp23@mail2.sysu.edu.cn">Email</a> &nbsp;/
                  <a href="https://scholar.google.com/citations?user=m8I1ELMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;&nbsp;
                  <a href="https://github.com/Hlings">Github</a>
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jonbarron/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/yipeng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yipeng_circle.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [07/01/2024] Two papers were accepted to ECCV 2024.
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [02/27/2024] One paper about 3D foundation models was accepted to CVPR 2024.
                </td>
              </tr>
            
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [09/21/2023] One co-author paper was accepted to NeurIPS 2023.
                </td>
              </tr>

              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [07/13/2023] One co-author paper was accepted to ICCV 2023.
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-bottom:10px;width:100%;vertical-align:middle">
                 [02/27/2023] The AsyFOD was accepted to CVPR 2023.
                </td>
              </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    <!-- <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0"> -->
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/mixcon3d.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/abs/2311.01734">Sculpting Holistic 3D Representation in Contrastive Language-Image-3D Pre-training</a> </papertitle>
                <br>
                    <strong>Yipeng Gao</strong>,
                    <a href="https://zw615.github.io/">Zeyu Wang</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>,
                    <a href="https://cihangxie.github.io/">Cihang Xie</a>,
                    <a href="https://yuyinzhou.github.io/">Yuyin Zhou</a>,
                <br>
                <em>CVPR</em>, 2024
                <br>
                </p>
                <div class="paper" id="gao2023asyfod">
                    <a href="https://arxiv.org/abs/2311.01734">paper</a> /
                    <a href="https://github.com/UCSC-VLAA/MixCon3D">code</a>
                </div>
                <br>
            </td>
        </tr> <!--gao2023MixCon3D-->
              
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/STDN_NeurIPS2023.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/abs/2310.17942">Diversifying Spatial-Temporal Perception for Video Domain Generalization</a></papertitle>
                <br>
                    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>,
                    Jia-Run Du,
                <strong>Yipeng Gao</strong>,
                    <a href="https://scholar.google.com/citations?hl=en&user=b3y40w8AAAAJ">Jiaming Zhou</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                </p>
                <div class="paper" id="stdn">
                    <a href="https://arxiv.org/abs/2310.17942">paper</a> /
                    <a href="https://github.com/KunyuLin/STDN/">code</a>
                </div>
                <br>
            </td>
            </tr> <!--kun2023STDN-->
      
            <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/asag.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/abs/2308.09242">ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation</a></papertitle>
                <br>
                    Shenghao Fu,
                    <a href="https://scholar.google.com/citations?user=QMm29SwAAAAJ&hl=en&oi=ao">Junkai Yan</a>,
                <strong>Yipeng Gao</strong>,
                    <a href="https://scholar.google.com/citations?user=5YZ3kvoAAAAJ&hl=en&oi=ao">Xiaohua Xie</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                </p>
                <div class="paper" id="fu2023ASAG">
                    <a href="https://arxiv.org/abs/2308.09242">paper</a> /
                    <a href="https://github.com/iSEE-Laboratory/ASAG">code</a>
                </div>
                <br>
            </td>
            </tr> <!--fu2023ASAG-->

        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/asyfod.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf">AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection</a> </papertitle>
                <br>
                    <strong>Yipeng Gao</strong>,
                    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>,
                    <a href="https://scholar.google.com/citations?user=QMm29SwAAAAJ&hl=en&oi=ao">Junkai Yan</a>,
                    <a href="https://scholar.google.com/citations?user=o_DllmIAAAAJ&hl=en&oi=ao">Yaowei Wang</a>,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>
                <br>
                <em>CVPR</em>, 2023
                <br>
                </p>
                <div class="paper" id="gao2023asyfod">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf">paper</a> /
                    <a href="https://github.com/Hlings/AsyFOD">code</a>
                </div>
                <br>
            </td>
        </tr> <!--gao2023AsyFOD-->

        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/acrofod.jpg' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                    <papertitle><a href="https://arxiv.org/pdf/2209.10904.pdf">AcroFOD: An Adaptive Method for Cross-domain Few-shot Object Detection</a> </papertitle>
                <br>
                    <strong>Yipeng Gao</strong>,
                    <a href="https://zjjconan.github.io/">Lingxiao Yang</a>,
                    Yunmu Huang,
                    Song Xie,
                    Shiyong Li,
                    <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=en&oi=ao">Wei-Shi Zheng</a>
                <br>
                <em>ECCV</em>, 2022
                <br>
                </p>
                <div class="paper" id="gao2023acrofod">
                    <a href="https://arxiv.org/pdf/2209.10904.pdf">paper</a> /
                    <a href="https://github.com/Hlings/AcroFOD">code</a>
                </div>
                <br>
            </td>
        </tr> <!--gao2023AsyFOD-->

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Academic Service</h2>
                  <p>
                      Conference Reviewer: ECCV 2024, ICML 2024, CVPR 2024, ICLR 2024, NeurIPS 2023
                  </p>
                  <p>
                      Journal Reviewer: TMLR
                  </p>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Awards</h2>
                  <p>
                      Chinese National Scholarship <a href="https://cse.sysu.edu.cn/content/6933">2023</a>
                  </p>
              </td>
            </tr>
          </tbody></table>
    
          <table width="100%" align="right border="0" cellpadding="20"><tbody>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last Update 4/2024. Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
